---
title: "Homework 2 - Data Mining"
author: "Max Machalek"
date: "February 28, 2019"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE}
library(ISLR)
library(FNN)
library(biotools)
library(car)
library(MASS)
titanic <- read.csv("//dellnas/mmachale/Folders/Downloads/titanic.csv", header = TRUE)
```
## H.2.1: Fit the first-oprder Logistic Regression model. ##  
  
(a) Perform the hypothesis test associated with Pclass at alpha = 0.05. Give the
hypotheses, test statistic, statistical decision, and practical conclusion of 
the test.  
  
Ho: Passenger Class has no effect on the survival rate of passengers. 
Ha: Passenger Class does have an effect on the survival rate of passengers.  
z: -5.833  
p: 5.44e-09 (=0.00000000544), (< 0.05)  
  
Because p < 0.05, we reject the null hypothesis and conclude that b1 is not 0. 
There is evidence at the 0.05 level of linear association in the population 
between the passenger class indicator and the odds of survival. We conclude
that the passenger class has a linear effect on probability of a passenger
to survive. 

  
(b) Interpret the odds estimate and the associated 95% confidence interval for 
Pclass.  
  
95% Interval for Odds Estimate: (0.24926793, 0.5013479)  
Odds Estimate: exp(-1.039840939) = 0.35351091  
We are 95% confident that the true value of the Pclass odds estimate is between 0.24926793 and 0.5013479.
  
95% Interval for Log Odds Estimate: (-1.389, -.690) 
Log Odds Estimate: -1.039840939  
We are 95% confident that the true value of the Pclass log odds estimate is between -1.389 and -0.690. 
 
  
The estimated log odds of survival is 1.039 lower for a passenger in a lower class compared to a passenger in the
class above them (3rd class being the 'lowest', 1st the 'highest'). So, the estimated odds of survival are 0.3535 times
as high for a passenger in one class than another passenger in the class above. 
 

(c) Give the confusion matrix and error rate using resubstitution. Explain what
the confusion matrix indicates about the types of prediction errors made by the 
logistic regression model.  
  
Sensitivity: 136/200 = 0.68 (pretty low)  
Specificity: 306/334 = 0.916 (not bad)  

Our confusion matrix shows a low sensitivity (0.68). Specificity is 0.916, which is quite a bit higher. This suggests
that the model is more likely to have false negative errors, which would be predicting that a passenger survives 
(a 0 value), when in actuality they do not (a 1 value). 


  $table
   y
yh    0   1
  0 306  64
  1  28 136

$err
[1] 0.1722846


```{r}
#err.f function
err.f = function(y,yh) {ct = table(yh,y)
  err = 1-sum(diag(ct))/length(y); list(table = ct, err = err)}

y = titanic$Survived
x1 <- titanic$Pclass
x2 <- titanic$Sex
x3 <- titanic$Age
x4 <- titanic$SibSp
x5 <- titanic$Parch
x6 <- titanic$Fare
x <- cbind(x1,x2,x3,x4,x5,x6)
standard.x <- scale(x)

n <- nrow(titanic)
  
lrFit <- glm(Survived~.,family=binomial(link="logit"), data = titanic) # logit link default
sfit <- summary(lrFit)

bh.o <- cbind(sfit$coefficients, confint.default(lrFit))
bh.o

exp(cbind(ORh=bh.o[,1],bh.o[,5:6]))[-1,]

#confusion matrix
p.lr = predict(lrFit, type="response")
yh.lr = as.numeric(p.lr>=0.5)
err.f(y,yh.lr)




```

## H.2.2 Fit all input variables for LDA, QDA, and KNN ## 
  
(a) Which variables are most important for discrimination with LDA? Explain  
  
  Sex has the largest impact on ability to discriminate, because magnitude of the
  scaled LD1 coefficient is the largest for this predictor (|-0.875| = 0.875). 
  
(b) Assess the assumption of equal population covariance matrices at alpha = 0.05. 
Give the statistical hypotheses, test statistic, statistical decision, and practical 
conclusion of the test. Should QDA predict better than LDA given this test result and 
the discussion in Section 4.5?  
  
  Ho: population covariance matrices do not differ.  
  Ha: Population covariance matrices are different.  
  c(21) = 291.65  
  p < 2e-16  
    
  Because c(21) = 291.65 and p < 2e-16, we reject the null hypothesis at the 0.05 level and conclude that
  there is evidence to question the assumption of equal population covariance matrices across default status.
  Additionally, the Cholesky residuals deviate quite a bit from the theoretical normal line, especially 
  toward the right tail. 
    
  Given this test result, we would expect QDA to predict better than LDA because its quadratic boundaries
  allow for a little more flexibility. 
  
(c) Compare the LOOCV error rates for LDA, QDA, and KNN (K = 5). Which method 
appears to be better in predicting Survival?  
  
LDA Error: 0.2097378
  
QDA Error: 0.2059925

KNN (k=5) Error: 0.2022472
  

  KNN (k=5) has the lowest LOOCV error rate. QDA has the second lowest, followed by LDA with the worst error rate.
  Given these error rates, KNN (k=5) appears to be the best method to predict survival. 


  .      xx1       xx2         xx3        xx4         xx5        xx6
LD1 -0.6826738 -2.158794 -0.02681108 -0.1865927 -0.09883254 0.00203539
LD1 -0.5466948 -0.874855 -0.35497800 -0.2050589 -0.07804787 0.09265525


LDA  
   y  
yh    0   1  
  0 287  65  
  1  47 135  

err: 0.2097378

QDA  
   y  
yh    0   1  
  0 288  64  
  1  46 136  

err: 0.2059925
```{r}
by(x,y,cov) # group covariance matrices #
boxM(x,y) # box M test #

mlm <- lm(x~y)
sp <- Manova(mlm)$SSPE/mlm$df.residual # pooled covariance matrix #
RM <- mlm$residuals%*%t(chol(solve(sp))) # Cholesky Residuals #
outlier.p <- c(exp=2*(1-pnorm(3)),obs=sum(abs(RM)>=3)/n) # 0.002699796, 0.069288390 #


#y for first plot is density instead of frequency like in notes, ?
par(mfrow=c(1,2))
hist(RM, main=NULL, freq=F, ylim=c(0,0.6), xlim=c(-3,6))
curve(dnorm,-4, 4, add=T, lty=2, col='red',lwd=2)
qqnorm(RM, pch=16, main=NULL); abline(0,1)

# model fit - LDA and QDA #
prior0 <- c(1,1)/2
da1 <-  lda(y~x, prior=prior0, CV=FALSE)
ld.raw <- da1$scaling
ld.std <- diag(sqrt(diag(sp)))%*%ld.raw
rbind(t(ld.raw),t(ld.std))

prior00 <- as.vector(table(y)/n)
CVS <- TRUE

da1 <- lda(y~x,prior=prior00,CV=TRUE)
da2 <- qda(y~x,prior=prior00,CV=TRUE)

da <- da1
if (CVS==F) yh.da = predict(da, as.data.frame(y))$class
if (CVS==T) yh.da = da$class
err.f(y,yh.da)

da <- da2
if (CVS==F) yh.da = predict(da, as.data.frame(y))$class
if (CVS==T) yh.da = da$class
err.f(y,yh.da)

## knn LOOCV error rate## 
yh.knn = knn.cv(standard.x,y,k=5) ## loocv

err.f(y,yh.knn)

```

